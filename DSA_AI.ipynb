{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5jH5-Uxt8PC"
      },
      "source": [
        "### **Cài đặt những thư viện cần thiết**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am2vlk2lX4Sg",
        "outputId": "74ca35eb-f70d-45bc-f307-c56851124e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.2.0-py3-none-any.whl.metadata (941 bytes)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.9-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (4.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.31.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.13.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.54b0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain-huggingface chromadb pymupdf transformers accelerate torch faiss-cpu networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbkGDDE0gyFl"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.llms import Ollama\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from langchain.schema import Document\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkW2RAUQYd0T"
      },
      "outputs": [],
      "source": [
        "loader = PyMuPDFLoader(\"/content/doc1.pdf\")\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "chunked_documents = text_splitter.split_documents(documents)\n",
        "db1 = Chroma.from_documents(chunked_documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT-IaP4Td25q"
      },
      "outputs": [],
      "source": [
        "loader = PyMuPDFLoader(\"/content/doc2.pdf\")\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "chunked_documents = text_splitter.split_documents(documents)\n",
        "db2 = Chroma.from_documents(chunked_documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNs48nOCd8eK"
      },
      "outputs": [],
      "source": [
        "loader = PyMuPDFLoader(\"/content/doc3.pdf\")\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        ")\n",
        "chunked_documents = text_splitter.split_documents(documents)\n",
        "db3 = Chroma.from_documents(chunked_documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FX9du-zmYUA6"
      },
      "outputs": [],
      "source": [
        "login(\"YOUR_HUGGING_FACE_KEY\")\n",
        "def load_llm(model_file):\n",
        "    # Load model và tokenizer\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_file,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_file,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=5000,\n",
        "        temperature=0.1,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    return llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3SChm2BYY77"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "retriever1 = db1.as_retriever(search_kwargs={\"k\": 4})\n",
        "retriever2 = db2.as_retriever(search_kwargs={\"k\": 4})\n",
        "retriever3 = db3.as_retriever(search_kwargs={\"k\": 4})\n",
        "\n",
        "llm = load_llm(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
        "\n",
        "rag_prompt = PromptTemplate.from_template(\n",
        "  \"\"\"\n",
        "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "    You are an AI assistant helping to answer questions based on provided contexts\n",
        "\n",
        "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "    Context: {context}\n",
        "    Question: {input}\n",
        "\n",
        "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "  \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "rewrite_prompt = PromptTemplate.from_template(\n",
        "  \"\"\"\n",
        "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "    You are an AI assistant helping to rewrite the given answer to make it cleaner, more structured, and easier to understand.\n",
        "\n",
        "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "    answer: [\n",
        "      Binary Search Trees (BSTs) are a type of binary tree where each node has a key and a value, and satisfies the restriction that the key in any node is larger than the keys in all nodes in its left subtree and smaller than the keys in all nodes in its right subtree.\n",
        "    ]\n",
        "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "    rewrited answer: [\n",
        "      A Binary Search Tree (BST) is a type of binary tree where each node contains a key and a value, and the tree follows a specific rule:\n",
        "\n",
        "        1. All keys in the left subtree of a node must be less than the node’s key.\n",
        "        2. All keys in the right subtree must be greater than the node’s key.\n",
        "        3. This rule is applied recursively to every node in the tree.\n",
        "\n",
        "      Because of this property, BSTs allow for efficient operations like searching, inserting, and deleting elements, usually with an average time complexity of O(log n) when the tree is balanced.\n",
        "    ]\n",
        "\n",
        "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "    answer: [\n",
        "      The text mentions the following sorting algorithms:\n",
        "        1. Counting Sort\n",
        "        2. Bucket Sort\n",
        "        3. Radix Sort\n",
        "        4. Bubble Sort\n",
        "        5. Insertion Sort\n",
        "    ]\n",
        "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "    rewrited answer: [\n",
        "      this is a few sorting algorithms:\n",
        "        1. Counting Sort\n",
        "        2. Bucket Sort\n",
        "        3. Radix Sort\n",
        "        4. Bubble Sort\n",
        "        5. Insertion Sort\n",
        "    ]\n",
        "\n",
        "\n",
        "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "    answer: [\n",
        "      {input}\n",
        "    ]\n",
        "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "  \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "rag_chain = LLMChain(llm=llm, prompt=rag_prompt)\n",
        "rewrite_chain = LLMChain(llm=llm, prompt=rewrite_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qejl4GDNipTM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "\n",
        "def extract_answer(text):\n",
        "    pattern = r\"<\\|eot_id\\|><\\|start_header_id\\|>assistant<\\|end_header_id\\|>\"\n",
        "    matches = list(re.finditer(pattern, text))\n",
        "    if matches:\n",
        "        last_match = matches[-1]\n",
        "        return text[last_match.end():].strip()\n",
        "    return None\n",
        "\n",
        "\n",
        "def query_rag_with_llm(rag_chain, rewrite_chain, question):\n",
        "    retrieved_docs = retriever1.get_relevant_documents(question)\n",
        "    retrieved_docs = [doc.page_content for doc in retrieved_docs]\n",
        "    result = rag_chain.invoke({\"input\": question, \"context\": retrieved_docs})\n",
        "\n",
        "\n",
        "    retrieved_docs = retriever2.get_relevant_documents(question)\n",
        "    retrieved_docs = [doc.page_content for doc in retrieved_docs]\n",
        "    retrieved_docs.append(extract_answer(result['text']))\n",
        "    result = rag_chain.invoke({\"input\": question, \"context\": retrieved_docs})\n",
        "\n",
        "\n",
        "    retrieved_docs = retriever3.get_relevant_documents(question)\n",
        "    retrieved_docs = [doc.page_content for doc in retrieved_docs]\n",
        "    retrieved_docs.append(extract_answer(result['text']))\n",
        "    result = rag_chain.invoke({\"input\": question, \"context\": retrieved_docs})\n",
        "\n",
        "\n",
        "    result = rewrite_chain.invoke({extract_answer(result['text'])})\n",
        "\n",
        "    return extract_answer(result['text'])\n",
        "question = \"What are the time complexities of common operations (insertion, deletion, search) in a binary search tree (BST), and how do they compare to those in a balanced binary search tree?\"\n",
        "print(query_rag_with_llm(rag_chain, rewrite_chain, question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIlX4N78oR75"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdh2Pg4CsS3d"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import conf, ngrok\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "# Bật lại event loop trong notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Ngrok token\n",
        "conf.get_default().auth_token = \"YOUR_NGROK_KEY\"\n",
        "\n",
        "# Khởi tạo FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Cho phép CORS để web gửi request được\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Dữ liệu gửi đến\n",
        "class Message(BaseModel):\n",
        "    message: str\n",
        "\n",
        "# Định nghĩa endpoint\n",
        "@app.post(\"/chat\")\n",
        "def chat_api(msg: Message):\n",
        "    prompt = msg.message\n",
        "    print(prompt)\n",
        "    output = query_rag_with_llm(rag_chain, rewrite_chain, prompt)  # Đã định nghĩa sẵn\n",
        "    return {\"reply\": output}\n",
        "\n",
        "# Tạo tunnel\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"🔗 API endpoint: {public_url}/chat\")\n",
        "\n",
        "# Chạy server\n",
        "uvicorn.run(app, port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
